---
title: "da410_project3"
author: "Jason Grahn"
date: "1/27/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
library(MASS)
```

#Problem 1.

## 1. Use admission.csv as a training dataset.

We're going to use `admission.csv` twice, so importing once and calling it good.
```{r import admission csv}
admission <- readr::read_csv(here::here("project3/admission.csv"), 
                             col_types = cols(De = col_factor(levels = c("admit", "border", "notadmit"))))

#plot scatter to look for clustering
admission %>% 
  ggplot(aes(x = GPA, y = GMAT, color = De)) +
  geom_point() +
  theme_classic() +
  theme(legend.position = "bottom")
```

## 2. Train model using LDA by setting admit/not-admit/border with the same probabilities.
```{r build the lda model}
model_1 <- lda(formula = De ~ ., 
                        data = admission, 
                        prior = c(1,1,1)/3)

model_1
```

## 3. Calculate the misclassfication rate
```{r}
#apply the model to admissions with predict()
predict_admit <- predict(model_1, # predictions
                data = admissions)

#pull the predictions and
predict_class <- predict_admit$class

#I might want to use that data later, so let's write them back to the table
admission_predictions <- admission %>% 
  mutate(predictions = predict_admit$class,
         LD1 = predict_admit$x[,1],
         LD2 = predict_admit$x[,2])

#back to misclassification
#quick glance of predictions versus actuals
table(predict_class, admission$De)

first_percentages <- admission_predictions %>% 
  mutate(correct_class = factor((De == predictions))) %>% 
  group_by(correct_class) %>% 
  summarise(classified_count = n(), 
            total = nrow(admission_predictions)) %>% 
  mutate(percentage = round(classified_count / total * 100, 2)) 

first_percentages
```

## 4. Predict students with GPA and SAT score as below.
```{r build a dataframe of the students to predict}
#the values
GPA <- c(3.14, 3.08, 2.08, 3.22)
GMAT <- c(470, 591, 641, 463)
p_student <- as.factor(c("student1", "student2", "student3", "student4"))

#put those in a table
predict_me <- tibble(p_student, GPA, GMAT)

#apply the model and save the output
predict_subgroup <- predict(model_1, # predictions
                            newdata = predict_me)

#incorporate the output into the original table
predict_me_output <- predict_me %>% 
  mutate(predictions = predict_subgroup$class)

predict_me_output
```

# Problem 2.

## 1. Use admission.csv as a training dataset.
We already input this before, so let's call it again just to show we have it. 
```{r}
head(admission,5)
```

## 2. Train model using LDA by setting probability of `admit` is 50% while probability of `not admit` is 25% and probability of `border` is 25%.
```{r probability change}
model_2 <- lda(formula = De ~ ., 
                        data = admission, 
                        prior = c(.5,.25,.25))

model_2
```

## 3. Calculate the misclassfication rate
```{r classification rate for heavier}
#apply the model to admissions with predict()
predict_admit_2 <- predict(model_2, # predictions
                data = admissions)

#pull the classificaton predictions into their own frame
predict_class_2 <- predict_admit_2$class

#I might want to use that data later, so let's write these back to the table as the other predictions
admission_predictions <- admission_predictions %>% 
  mutate(predictions_2 = predict_admit_2$class,
         LD1_2 = predict_admit_2$x[,1],
         LD2_2 = predict_admit_2$x[,2])

#back to misclassification
#quick glance of predictions versus actuals
table(predict_class_2, admission$De)

second_percentages <- admission_predictions %>% 
  mutate(correct_class_2 = factor((De == predictions_2))) %>% 
  group_by(correct_class_2) %>% 
  summarise(classified_count = n(), 
            total = nrow(admission_predictions)) %>% 
  mutate(percentage = round(classified_count / total * 100, 2)) 
```

## 4. Predict students with GPA and SAT score as below.
```{r second prediction of small student group}
#apply the model and save the output
predict_subgroup_2 <- predict(model_2, # predictions
                            newdata = predict_me)

#incorporate the output into the previous prediction table
predict_me_output <- predict_me_output %>% 
  mutate(predictions_2 = predict_subgroup_2$class)

predict_me_output
```

## Compare differences of the result from problem1.
```{r}
#One way to compare results is to look at the difference in percentages between missclassfied students

# a negative number indicates that the second model was worse than the first. 
# ie; the second model incorrectly predicted a larger count than the first.

#a positive number indicates that the second model was better, that it more accurately predicted the actual
first_percentages$percentage[1] - second_percentages$percentage[1]
```

We see that the second model decreased the misclassification rate by `r first_percentages$percentage[1] - second_percentages$percentage[1]` points; that it accurately predicted the actual better than the first model. 
```{r}
first_percentages %>% 
  dplyr::select(correct_class, percentage) %>% 
  rename(first_model = percentage) %>% 
  mutate(second_model = (second_percentages$percentage),
         correct_class = factor(correct_class, levels = c(TRUE, FALSE))) %>% 
  gather(key = sample, value = measurement, -correct_class) %>% 
  ggplot(aes(x = sample, y = measurement, label = measurement)) +
 # scale_x_discrete(position = "top") +
  geom_col(aes(fill = correct_class)) +
  theme_classic() + 
  geom_label(vjust = "bottom") +
  labs(title = "Percentages between accuracies between models",
       subtitle = "TRUE predictions showed improvement between the first and second model.")
```

We should check for equal covariance just to make sure we're good.

```{r}
heplots::boxM(cbind(GPA, GMAT) ~ De, 
              data = admission)
```

With a p-value of `r heplots::boxM(cbind(GPA, GMAT) ~ De, data = admission)[3]` we can be assured that the test of homogeneity of covariance passes, that the values are not covariant.

# Problem 3.

## Explain what is Quadratic Discriminant Analysis (QDA), and use QDA to train the model, discuss if this project can be done better by QDA, why or why not. 

`QDA`, 

```{r train with qda}
model_qda <- MASS::qda(formula = De ~ ., 
                       data = admission, 
                       prior = c(.5,.25,.25))
#apply the QDA model to admissions with predict()
predict_qda <- predict(model_qda, # predictions
                           data = admissions)

#pull the classificaton predictions into their own frame
predict_qda_output <- predict_qda$class

#I might want to use that data later, so let's write these back to the table as the other predictions
admission_predictions <- admission_predictions %>% 
  mutate(predictions_qda = predict_qda_output)

#back to misclassification
#quick glance of predictions versus actuals
#table(predict_qda_output, admission$De)

third_percentages <- admission_predictions %>% 
  mutate(correct_class_3 = factor((De == predictions_qda))) %>% 
  group_by(correct_class_3) %>% 
  summarise(classified_count = n(), 
            total = nrow(admission_predictions)) %>% 
  mutate(percentage = round(classified_count / total * 100, 2)) 

knitr::kable(third_percentages) %>%
  kableExtra::kable_styling(bootstrap_options = "striped")
```

I believe that LDA (method 2) is the better method to use for this project. While it does have a more accurate classification rate, given the sample size and quantity of variables, QDA is potentially over fitting the model to the data. 
